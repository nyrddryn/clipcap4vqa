{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PYTHONPATH=.\n",
    "export AOKVQA_DIR=./datasets/aokvqa/\n",
    "export COCO_DIR=./datasets/coco/\n",
    "export FEATURES_DIR=./features/\n",
    "export LOG_DIR=./logs/\n",
    "export PREDS_DIR=./predictions/\n",
    "export PT_MODEL_DIR=./pretrained_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import clip\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from load_aokvqa import load_aokvqa\n",
    "from evaluation.remap_predictions import map_to_choices\n",
    "from ClipCap.data import prompt_text, load_data\n",
    "from ClipCap.train import load_config, load_model\n",
    "import skimage.io as io\n",
    "import PIL.Image\n",
    "from IPython.display import Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_model() missing 1 required positional argument: 'cfg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_138100/1157670113.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logs/clipcap-ra/checkpoints/ckpt-010.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: load_model() missing 1 required positional argument: 'cfg'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--log-dir', type=pathlib.Path, required=True, dest='log_dir')\n",
    "    parser.add_argument('--epoch', type=int, required=True)\n",
    "    parser.add_argument('--aokvqa-dir', type=pathlib.Path, required=True, dest='aokvqa_dir')\n",
    "    parser.add_argument('--split', type=str, choices=['train', 'val', 'test'], required=True)\n",
    "    parser.add_argument('--eval-features', type=pathlib.Path, required=True, dest='eval_features')\n",
    "    parser.add_argument('--map-to-choices', action='store_true', dest='map_to_choices')\n",
    "    parser.add_argument('--beam-search', action='store_true', dest='beam_search')\n",
    "    parser.add_argument('--out', type=argparse.FileType('w'), required=True, dest='output_file')\n",
    "    cfg = parser.parse_args()\n",
    "\n",
    "    cfg = argparse.Namespace(\n",
    "        **vars(cfg),\n",
    "        **vars(load_config(os.path.join(cfg.log_dir, 'model_config.json'))),\n",
    "        **{\n",
    "            'finetune_gpt': False,\n",
    "            f\"{cfg.split}_features\": cfg.eval_features\n",
    "        }\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = load_model(\n",
    "        cfg,\n",
    "        os.path.join(cfg.log_dir, 'checkpoints', f\"ckpt-{cfg.epoch:03d}.pt\")\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    ## Run inference\n",
    "    predictions = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            q = dataset.question_ids[i]\n",
    "            prefix, prompt_tokens, prompt_len = dataset[i]\n",
    "\n",
    "            prefix = prefix.unsqueeze(0).to(device)\n",
    "            prompt_tokens = prompt_tokens.unsqueeze(0).to(device)\n",
    "            embedding_text = model.gpt.transformer.wte(prompt_tokens)\n",
    "            prefix_projections = model.clip_project(prefix).view(-1, model.prefix_length, model.gpt_embedding_size)\n",
    "            embed = torch.cat(( prefix_projections, embedding_text ), dim=1)\n",
    "\n",
    "            if cfg.beam_search:\n",
    "                generated_text = generate_beam(model, tokenizer, embed, device, beam_size=5, return_top_pred=True, entry_length=entry_length, stop_token_index=tokenizer.eos_token_id)\n",
    "            else:\n",
    "                generated_text = generate(model, tokenizer, embed=embed, entry_length=entry_length, stop_token_index=tokenizer.eos_token_id)\n",
    "\n",
    "            predictions[q] = generated_text\n",
    "\n",
    "    if cfg.map_to_choices:\n",
    "        aokvqa_set = load_aokvqa(cfg.aokvqa_dir, cfg.split)\n",
    "        aokvqa_set = { aokvqa_set[i]['question_id'] : aokvqa_set[i] for i in range(len(aokvqa_set)) }\n",
    "        predictions = map_to_choices(aokvqa_set, predictions)\n",
    "\n",
    "    json.dump(predictions, cfg.output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \" \" # linkimage\n",
    "image = io.imread(url)\n",
    "pil_image = PIL.Image.fromarray(image)\n",
    "\n",
    "\n",
    "image = preprocess(pil_image).unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aokvqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
